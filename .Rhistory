labs(title = "Num Plays per Month vs Difficulty",
colour = "Difficulty")
ggsave("output/ Num Plays per Month Difficulty.pdf")
# year published
ggplot(games_cleaned, aes(yearpublished, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "BuPu") +
labs(title = "Year Published vs Difficulty", colour = "Difficulty")
ggsave("output/ Year Publish Difficulty.pdf")
# curious about scatterplot with target and podcast
ggplot(games_cleaned, aes(podcast, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "BuPu") +
labs(title = "Podcasts vs Difficulty", colour = "Difficulty")
ggsave("output/ Podcast Difficulty.pdf")
# curious about scatterplot with target and news
ggplot(games_cleaned, aes(news, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "BuPu") +
labs(title = "News vs Difficulty", colour = "Difficulty")
ggsave("output/ News Difficulty.pdf")
# -- Multivariate plots of x var and y var ---
ggplot(games_cleaned, aes(minplaytime, fill = difficulty)) +
geom_histogram(position = "fill") +
scale_fill_brewer(palette = "BuPu") +
labs(title = "Min Playtime Distribution by Difficulty",
fill = "Difficulty")
ggsave("output/hist of minplaytime.pdf")
ggplot(games_cleaned, aes(desc_word_count, numwanting, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "BuPu") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
ggsave("output/ Word Count Difficulty.pdf")
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# exploratory analysis!
# look at target
games_cleaned %>% glimpse() %>% summary()
ggplot(games_cleaned, aes(difficulty, fill = difficulty)) +
geom_bar() +
scale_fill_brewer(palette = "Set2") +
labs(title = "Count of Complex vs Simple Games",
fill = "Difficulty")
ggsave("output/ComplexXSimple.pdf")
games_cleaned %>%
select(minplayers, maxplayers, minplaytime, maxplaytime, yearpublished, desc_word_count) %>%
gather() %>%
ggplot(aes(value)) +
geom_histogram(bins = 30, fill = "grey40") +
facet_wrap(~ key, scales = "free") +
labs(title = "Numeric Variable Distributions")
ggsave("output/Numeric Variable Distributions.pdf")
ggplot(games_cleaned, aes(difficulty, minplaytime, fill = difficulty)) +
geom_boxplot(alpha = 0.8) +
scale_fill_brewer(palette = "Set2") +
labs(title = "Min Playtime by Difficulty", fill = "Difficulty")
ggsave("output/Min Playtime by Difficulty.pdf")
# histograms
ggplot(games_cleaned, aes(yearpublished, fill = difficulty)) +
geom_bar(position = "fill") +
scale_fill_brewer(palette = "Set2") +
labs(title = "Difficulty Proportions Over Years",
fill = "Difficulty")
ggsave("output/Year Published Difficulty.pdf")
# this one is kinda cool to look at especially looking at years after 1950 and the visual of the graph
# i don't think it really tells us a whole lot but just gives another way to understand the data
ggplot(games_cleaned, aes(numplays_month, fill = difficulty)) +
geom_histogram(position = "fill", binwidth = 150) +
scale_fill_brewer(palette = "Set2") +
labs(title = "Monthly Plays by Difficulty",
fill = "Difficulty")
ggsave("output/Num Plays per Month Difficulty Hist.pdf")
# this also gives us a glimpse into the data and the numplays per month and it's interesting to see
# the bands
ggplot(games_cleaned, aes(numwanting, fill = difficulty)) +
geom_histogram(position = "fill", binwidth = 75) +
scale_fill_brewer(palette = "Set2") +
labs(title = "Number Wanting by Difficulty",
fill = "Difficulty")
ggsave("output/Num Wanting Difficulty Hist.pdf")
# more plots!
# come back to make colorblindness friendly
# chat helped to make pretty for now
# see relationship between difficulty and numwanting
ggplot(games_cleaned, aes(difficulty, numwanting, colour = difficulty)) +
geom_boxplot(alpha = 0.7) +
scale_colour_brewer(palette = "Set2") +
scale_y_log10() +
labs(title = "Log NumWanting by Difficulty",
colour = "Difficulty")
ggsave("output/ Num Wanting Difficulty Boxplot.pdf")
# see relationship between difficulty and numplays
ggplot(games_cleaned, aes(numplays, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
labs(title = "Num Plays vs Difficulty", colour = "Difficulty")
ggsave("output/ Num Plays Difficulty.pdf")
ggplot(games_cleaned, aes(numplays_month, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
labs(title = "Num Plays per Month vs Difficulty",
colour = "Difficulty")
ggsave("output/ Num Plays per Month Difficulty.pdf")
# year published
ggplot(games_cleaned, aes(yearpublished, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
labs(title = "Year Published vs Difficulty", colour = "Difficulty")
ggsave("output/ Year Publish Difficulty.pdf")
# curious about scatterplot with target and podcast
ggplot(games_cleaned, aes(podcast, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
labs(title = "Podcasts vs Difficulty", colour = "Difficulty")
ggsave("output/ Podcast Difficulty.pdf")
# curious about scatterplot with target and news
ggplot(games_cleaned, aes(news, difficulty, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
labs(title = "News vs Difficulty", colour = "Difficulty")
ggsave("output/ News Difficulty.pdf")
# -- Multivariate plots of x var and y var ---
ggplot(games_cleaned, aes(minplaytime, fill = difficulty)) +
geom_histogram(position = "fill") +
scale_fill_brewer(palette = "Set2") +
labs(title = "Min Playtime Distribution by Difficulty",
fill = "Difficulty")
ggsave("output/hist of minplaytime.pdf")
ggplot(games_cleaned, aes(desc_word_count, numwanting, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
ggsave("output/ Word Count Difficulty.pdf")
# -- Multivariate plots of x var and y var ---
ggplot(games_cleaned, aes(minplaytime, fill = difficulty)) +
geom_histogram(position = "fill") +
scale_fill_brewer(palette = "Set2") +
labs(title = "Min Playtime Distribution by Difficulty",
fill = "Difficulty")
ggplot(games_cleaned, aes(desc_word_count, numwanting, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
ggplot(games_cleaned, aes(minplayers, languagedependence, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
ggplot(games_cleaned, aes(minplayers, minage, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
ggplot(games_cleaned, aes(minplayers, numplays_month, colour = difficulty)) +
geom_point(alpha = 0.6) +
scale_colour_brewer(palette = "Set2") +
theme_bw() +
labs(title = "Word Count vs Number Wanting",
colour = "Difficulty")
# STAT 172 Final Project
# erika, phoebe, kaitlyn
rm(list = ls())
library(ggplot2)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(tidymodels)
library(randomForest)
library(RColorBrewer)
library(glmnet)
library(lubridate)
library(reshape2)
library(stringr)
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
# Goal: predict whether a board game (row in the data) is difficult based on rating of complexity (categorical response)
# this will help us to find a simple model :)
# for lasso and ridge we need binary RV
games_cleaned$difficulty <- ifelse(games_cleaned$difficulty == "Complex", 1, 0)
games_cleaned_lr <- games_cleaned
# ---> split into test/train
RNGkind(sample.kind = "default")
set.seed(44446)
train.idx <- sample(x= 1:nrow(games_cleaned_lr), size = .7*nrow(games_cleaned_lr))
train.df <- games_cleaned_lr[train.idx,]
test.df <- games_cleaned_lr[-train.idx,]
# start w/traditional logistic regression model fit with MLE
lr_mle <- glm(difficulty ~ .,
data = train.df,
family = binomial(link = "logit"))
# MODELING TIME
games_cleaned %>% glimpse() %>% summary() %>% view()
%>% view()
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
games_cleaned <- games_cleaned %>% select(-c(1))
games_cleaned %>% glimpse()
# Goal: predict whether a board game (row in the data) is difficult based on rating of complexity (categorical response)
# this will help us to find a simple model :)
# for lasso and ridge we need binary RV
games_cleaned$difficulty <- ifelse(games_cleaned$difficulty == "Complex", 1, 0)
games_cleaned_lr <- games_cleaned
# ---> split into test/train
RNGkind(sample.kind = "default")
set.seed(44446)
train.idx <- sample(x= 1:nrow(games_cleaned_lr), size = .7*nrow(games_cleaned_lr))
train.df <- games_cleaned_lr[train.idx,]
test.df <- games_cleaned_lr[-train.idx,]
# start w/traditional logistic regression model fit with MLE
lr_mle <- glm(difficulty ~ .,
data = train.df,
family = binomial(link = "logit"))
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
games_cleaned <- games_cleaned %>% select(-c(1))
games_cleaned %>% glimpse()
# Goal: predict whether a board game (row in the data) is difficult based on rating of complexity (categorical response)
# this will help us to find a simple model :)
# for lasso and ridge we need binary RV
games_cleaned$difficulty <- ifelse(games_cleaned$difficulty == "Complex", 1, 0)
games_cleaned_lr <- games_cleaned
# ---> split into test/train
RNGkind(sample.kind = "default")
set.seed(2291352)
train.idx <- sample(x= 1:nrow(games_cleaned_lr), size = .7*nrow(games_cleaned_lr))
train.df <- games_cleaned_lr[train.idx,]
test.df <- games_cleaned_lr[-train.idx,]
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
source("~/Desktop/STAT 172/GitHub/STAT-172-Final/src/Board Games Data Cleaning.R", echo = TRUE)
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
source("~/Desktop/STAT 172/GitHub/STAT-172-Final/src/Board Games Data Cleaning.R", echo = TRUE)
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
games_cleaned <- games_cleaned %>% select(-c(1))
games_cleaned %>% glimpse()
# Goal: predict whether a board game (row in the data) is difficult based on rating of complexity (categorical response)
# this will help us to find a simple model :)
# for lasso and ridge we need binary RV
games_cleaned$difficulty <- ifelse(games_cleaned$difficulty == "Complex", 1, 0)
games_cleaned_lr <- games_cleaned
# ---> split into test/train
RNGkind(sample.kind = "default")
set.seed(2291352)
train.idx <- sample(x= 1:nrow(games_cleaned_lr), size = .7*nrow(games_cleaned_lr))
train.df <- games_cleaned_lr[train.idx,]
test.df <- games_cleaned_lr[-train.idx,]
# start w/traditional logistic regression model fit with MLE
lr_mle <- glm(difficulty ~ .,
data = train.df,
family = binomial(link = "logit"))
lr_mle_coefs <- coef(lr_mle)
# build X matrices for lasso, ridge regression
x.train <- model.matrix(difficulty ~ ., data = train.df)[,-1]
x.test <- model.matrix(difficulty ~ ., data = test.df)[,-1]
# "vectorized" y vectors
y.train <- as.vector(train.df$difficulty)
y.test <- as.vector(test.df$difficulty)
# Use cross validation to quickly fit & assess many many lasso & rid"ge regression models
lr_lasso_cv <- cv.glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 1)
lr_ridge_cv <- cv.glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 0)
plot(lr_lasso_cv, sign.lambda = 1)
plot(lr_ridge_cv, sign.lambda = 1)
best_lasso_lambda <- lr_lasso_cv$lambda.min # this is the lambda that minimizes out of sample error (based on cross validation)
best_ridge_lambda <- lr_ridge_cv$lambda.min
# these would be the lambdas that would be used to fit "the predictive model"
# we can also look at the coeffs from the best models
lr_ridge_coefs <- coef(lr_ridge_cv, s = best_ridge_lambda)
lr_lasso_coefs <- coef(lr_lasso_cv, s = best_lasso_lambda)
# just to understand, let's plot the coeffs against each other
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_ridge_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_lasso_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
# just to understand, let's plot the coeffs against each other
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_ridge_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_lasso_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
# fit those final models
final_lasso <- glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 1,
lambda = best_lasso_lambda)
final_ridge <- glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 0,
lambda = best_ridge_lambda)
test.df.preds <- test.df %>%
mutate(mle_pred = predict(lr_mle, test.df, type = "response"), # gives probability of Y = 1
lasso_pred = predict(final_lasso, x.test, type = "response")[,1],
ridge_pred = predict(final_ridge, x.test, type = "response")[,1])
cor(test.df.preds$mle_pred, test.df.preds$lasso_pred)
plot(test.df.preds$mle_pred, test.df.preds$lasso_pred)
# make ROC curves for each of the three models
mle_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$mle_pred,
levels = c("0", "1"))
ridge_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$ridge_pred,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$lasso_pred,
levels = c("0", "1"))
#make data frame of MLE ROC info
mle_data <- data.frame(
Model = "MLE",
Specificity = mle_rocCurve$specificities,
Sensitivity = mle_rocCurve$sensitivities,
AUC = as.numeric(mle_rocCurve$auc)
)
#make data frame of lasso ROC info
lasso_data <- data.frame(
Model = "Lasso",
Specificity = lasso_rocCurve$specificities,
Sensitivity = lasso_rocCurve$sensitivities,
AUC = lasso_rocCurve$auc %>% as.numeric
)
#make data frame of ridge ROC info
ridge_data <- data.frame(
Model = "Ridge",
Specificity = ridge_rocCurve$specificities,
Sensitivity = ridge_rocCurve$sensitivities,
AUC = ridge_rocCurve$auc%>% as.numeric
)
# Combine all the data frames
roc_data <- rbind(mle_data, lasso_data, ridge_data)
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
aes(x = 0.75, y = c(0.75, 0.65, 0.55), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
ggplot(games_cleaned, aes(x = difficulty, y = avgweight)) +
geom_boxplot()
ggplot(games_cleaned, aes(x = difficulty, y = avgweight)) +
geom_boxplot()
games %>% glimpse()
games <- games %>% select(-c(10))
games %>% glimpse()
source("~/Desktop/STAT 172/GitHub/STAT-172-Final/src/Board Games Exploratory Analysis.R", echo = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
write.csv(games,"data/cleanboardgames.csv", row.names = FALSE)
# STAT 172 Final Project
# erika, phoebe, kaitlyn
rm(list = ls())
library(ggplot2)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(tidymodels)
library(randomForest)
library(RColorBrewer)
library(glmnet)
library(lubridate)
library(reshape2)
library(stringr)
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
source("~/Desktop/STAT 172/GitHub/STAT-172-Final/src/Board Games Data Cleaning.R", echo = TRUE)
# STAT 172 Final Project
# erika, phoebe, kaitlyn
rm(list = ls())
library(ggplot2)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(pROC)
library(tidymodels)
library(randomForest)
library(RColorBrewer)
library(glmnet)
library(lubridate)
library(reshape2)
library(stringr)
games_cleaned <- read.csv("data/cleanboardgames.csv", stringsAsFactors = TRUE)
# MODELING TIME
games_cleaned %>% glimpse() %>% summary()
games_cleaned <- games_cleaned %>% select(-c(1))
games_cleaned %>% glimpse()
# Goal: predict whether a board game (row in the data) is difficult based on rating of complexity (categorical response)
# this will help us to find a simple model :)
# for lasso and ridge we need binary RV
games_cleaned$difficulty <- ifelse(games_cleaned$difficulty == "Complex", 1, 0)
games_cleaned_lr <- games_cleaned
# ---> split into test/train
RNGkind(sample.kind = "default")
set.seed(2291352)
train.idx <- sample(x= 1:nrow(games_cleaned_lr), size = .7*nrow(games_cleaned_lr))
train.df <- games_cleaned_lr[train.idx,]
test.df <- games_cleaned_lr[-train.idx,]
# start w/traditional logistic regression model fit with MLE
lr_mle <- glm(difficulty ~ .,
data = train.df,
family = binomial(link = "logit"))
lr_mle_coefs <- coef(lr_mle)
# build X matrices for lasso, ridge regression
x.train <- model.matrix(difficulty ~ ., data = train.df)[,-1]
x.test <- model.matrix(difficulty ~ ., data = test.df)[,-1]
# "vectorized" y vectors
y.train <- as.vector(train.df$difficulty)
y.test <- as.vector(test.df$difficulty)
# Use cross validation to quickly fit & assess many many lasso & rid"ge regression models
lr_lasso_cv <- cv.glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 1)
lr_ridge_cv <- cv.glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 0)
plot(lr_lasso_cv, sign.lambda = 1)
plot(lr_ridge_cv, sign.lambda = 1)
best_lasso_lambda <- lr_lasso_cv$lambda.min # this is the lambda that minimizes out of sample error (based on cross validation)
best_ridge_lambda <- lr_ridge_cv$lambda.min
# these would be the lambdas that would be used to fit "the predictive model"
# we can also look at the coeffs from the best models
lr_ridge_coefs <- coef(lr_ridge_cv, s = best_ridge_lambda)
lr_lasso_coefs <- coef(lr_lasso_cv, s = best_lasso_lambda)
# just to understand, let's plot the coeffs against each other
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_ridge_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
ggplot() +
geom_point(aes(as.vector(lr_mle_coefs), as.vector(lr_lasso_coefs))) +
geom_abline(aes(slope = 1, intercept = 0)) +
xlim(c(-10,10)) + ylim(c(-10,10))
# fit those final models
final_lasso <- glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 1,
lambda = best_lasso_lambda)
final_ridge <- glmnet(x.train, y.train, family = binomial(link = "logit"), alpha = 0,
lambda = best_ridge_lambda)
test.df.preds <- test.df %>%
mutate(mle_pred = predict(lr_mle, test.df, type = "response"), # gives probability of Y = 1
lasso_pred = predict(final_lasso, x.test, type = "response")[,1],
ridge_pred = predict(final_ridge, x.test, type = "response")[,1])
cor(test.df.preds$mle_pred, test.df.preds$lasso_pred)
plot(test.df.preds$mle_pred, test.df.preds$lasso_pred)
# make ROC curves for each of the three models
mle_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$mle_pred,
levels = c("0", "1"))
ridge_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$ridge_pred,
levels = c("0", "1"))
lasso_rocCurve <- roc(response = as.factor(test.df.preds$difficulty),
predictor = test.df.preds$lasso_pred,
levels = c("0", "1"))
#make data frame of MLE ROC info
mle_data <- data.frame(
Model = "MLE",
Specificity = mle_rocCurve$specificities,
Sensitivity = mle_rocCurve$sensitivities,
AUC = as.numeric(mle_rocCurve$auc)
)
#make data frame of lasso ROC info
lasso_data <- data.frame(
Model = "Lasso",
Specificity = lasso_rocCurve$specificities,
Sensitivity = lasso_rocCurve$sensitivities,
AUC = lasso_rocCurve$auc %>% as.numeric
)
#make data frame of ridge ROC info
ridge_data <- data.frame(
Model = "Ridge",
Specificity = ridge_rocCurve$specificities,
Sensitivity = ridge_rocCurve$sensitivities,
AUC = ridge_rocCurve$auc%>% as.numeric
)
# Combine all the data frames
roc_data <- rbind(mle_data, lasso_data, ridge_data)
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
aes(x = 0.75, y = c(0.75, 0.65, 0.55), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
ggplot(games_cleaned, aes(x = difficulty, y = avgweight)) +
geom_boxplot()
# Plot the data
ggplot() +
geom_line(aes(x = 1 - Specificity, y = Sensitivity, color = Model),data = roc_data) +
geom_text(data = roc_data %>% group_by(Model) %>% slice(1),
aes(x = 0.75, y = c(0.75, 0.65, 0.55), colour = Model,
label = paste0(Model, " AUC = ", round(AUC, 3)))) +
scale_colour_brewer(palette = "Paired") +
labs(x = "1 - Specificity", y = "Sensitivity", color = "Model") +
theme_minimal()
